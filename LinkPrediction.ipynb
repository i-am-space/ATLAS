{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINK PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODE2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 249/249 [00:00<00:00, 1132.19it/s]\n",
      "Generating walks (CPU: 4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:41<00:00,  1.21it/s]\n",
      "Generating walks (CPU: 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:42<00:00,  1.19it/s]\n",
      "Generating walks (CPU: 2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:42<00:00,  1.17it/s]\n",
      "Generating walks (CPU: 3): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:42<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Node2Vec + Letter Features Accuracy: 0.8291\n",
      "üîπ Node2Vec + Letter Features ROC-AUC Score: 0.8692\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from node2vec import Node2Vec\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load country data\n",
    "file_path = \"./data/country.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract country names\n",
    "countries = list(df['value'])\n",
    "\n",
    "# Create directed graph\n",
    "G = nx.DiGraph()\n",
    "for country in countries:\n",
    "    last_letter = country[-1].lower()\n",
    "    for other_country in countries:\n",
    "        if other_country[0].lower() == last_letter and country != other_country:\n",
    "            G.add_edge(country, other_country)\n",
    "\n",
    "# Step 1: Generate Node2Vec Embeddings\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=50, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Get Node2Vec embedding for a node\n",
    "def get_embedding(node):\n",
    "    return model.wv[node] if node in model.wv else np.zeros(64)\n",
    "\n",
    "# Step 2: One-Hot Encode First & Last Letters\n",
    "letters = sorted(set([c[0].lower() for c in countries] + [c[-1].lower() for c in countries]))\n",
    "letter_encoder = OneHotEncoder(sparse_output=False, categories=[letters, letters])\n",
    "\n",
    "# Fit OneHotEncoder\n",
    "letter_encoder.fit([[c[0].lower(), c[-1].lower()] for c in countries])\n",
    "\n",
    "# Convert each country to (first_letter, last_letter) encoding\n",
    "def get_letter_feature(country):\n",
    "    return letter_encoder.transform([[country[0].lower(), country[-1].lower()]])[0]\n",
    "\n",
    "# Step 3: Create Feature Vectors for Edges\n",
    "def create_edge_features(edges):\n",
    "    edge_features = []\n",
    "    for u, v in edges:\n",
    "        emb_u, emb_v = get_embedding(u), get_embedding(v)\n",
    "        letter_feat_u, letter_feat_v = get_letter_feature(u), get_letter_feature(v)\n",
    "        edge_features.append(np.concatenate([emb_u, emb_v, letter_feat_u, letter_feat_v]))\n",
    "    return np.array(edge_features)\n",
    "\n",
    "# Step 4: Mask Some Edges (20% of real edges for testing)\n",
    "edges = list(G.edges)\n",
    "random.shuffle(edges)\n",
    "split = int(len(edges) * 0.8)\n",
    "train_edges, test_edges = edges[:split], edges[split:]\n",
    "\n",
    "# Remove test edges from the graph (Masking)\n",
    "G_train = G.copy()\n",
    "G_train.remove_edges_from(test_edges)\n",
    "\n",
    "# Step 5: Sample Negative Edges (Non-Existing Links)\n",
    "non_edges = list(nx.non_edges(G))\n",
    "random.shuffle(non_edges)\n",
    "train_non_edges = non_edges[:split]\n",
    "test_non_edges = non_edges[split:]\n",
    "\n",
    "# Ensure we have positive and negative samples\n",
    "X_train = np.vstack((create_edge_features(train_edges), create_edge_features(train_non_edges)))\n",
    "y_train = np.hstack((np.ones(len(train_edges)), np.zeros(len(train_non_edges))))  # 1 = real link, 0 = fake link\n",
    "\n",
    "X_test = np.vstack((create_edge_features(test_edges), create_edge_features(test_non_edges)))\n",
    "y_test = np.hstack((np.ones(len(test_edges)), np.zeros(len(test_non_edges))))  # 1 = real link, 0 = fake link\n",
    "\n",
    "# Step 6: Train a Logistic Regression Model for Link Prediction\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the Model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"üîπ Node2Vec + Letter Features Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üîπ Node2Vec + Letter Features ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 249 nodes and 6878 edges.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx, negative_sampling\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load country data\n",
    "file_path = \"./data/country.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract country names\n",
    "countries = list(df['value'])\n",
    "\n",
    "# Create directed graph\n",
    "G = nx.DiGraph()\n",
    "for country in countries:\n",
    "    last_letter = country[-1].lower()\n",
    "    for other_country in countries:\n",
    "        if other_country[0].lower() == last_letter and country != other_country:\n",
    "            G.add_edge(country, other_country)\n",
    "\n",
    "# Convert NetworkX graph to PyTorch Geometric format\n",
    "G_undirected = G.to_undirected()\n",
    "data = from_networkx(G_undirected)\n",
    "\n",
    "# Step 1: One-Hot Encode First & Last Letters\n",
    "letters = sorted(set([c[0].lower() for c in countries] + [c[-1].lower() for c in countries]))  # Unique letters\n",
    "letter_encoder = OneHotEncoder(sparse_output=False, categories=[letters, letters])\n",
    "\n",
    "# Fit OneHotEncoder\n",
    "letter_encoder.fit([[c[0].lower(), c[-1].lower()] for c in countries])\n",
    "\n",
    "# Convert each country to (first_letter, last_letter) encoding\n",
    "def get_letter_feature(country):\n",
    "    return letter_encoder.transform([[country[0].lower(), country[-1].lower()]])[0]\n",
    "\n",
    "# Assign node features (first & last letter encoding)\n",
    "node_features = np.array([get_letter_feature(c) for c in countries])\n",
    "data.x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "print(f\"Graph has {data.num_nodes} nodes and {data.num_edges} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Define Graph Convolutional Network (GCN) Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Mask out some edges for training\n",
    "num_edges = data.edge_index.shape[1]\n",
    "perm = torch.randperm(num_edges)\n",
    "train_mask = perm[:int(0.8 * num_edges)]\n",
    "test_mask = perm[int(0.8 * num_edges):]\n",
    "\n",
    "train_edges = data.edge_index[:, train_mask]\n",
    "test_edges = data.edge_index[:, test_mask]\n",
    "\n",
    "# Generate negative (non-existent) edges\n",
    "neg_edges = negative_sampling(data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_edges)\n",
    "\n",
    "train_neg_edges = neg_edges[:, :train_edges.shape[1]]\n",
    "test_neg_edges = neg_edges[:, test_edges.shape[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3796\n",
      "Epoch 20, Loss: 1.1289\n",
      "Epoch 40, Loss: 0.9574\n",
      "Epoch 60, Loss: 0.8345\n",
      "Epoch 80, Loss: 0.8036\n",
      "Epoch 100, Loss: 0.7882\n",
      "Epoch 120, Loss: 0.7670\n",
      "Epoch 140, Loss: 0.7611\n",
      "Epoch 160, Loss: 0.7547\n",
      "Epoch 180, Loss: 0.7512\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model & Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GCN(in_channels=data.x.shape[1], hidden_channels=32, out_channels=16).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Move data to GPU if available\n",
    "data = data.to(device)\n",
    "train_edges, train_neg_edges = train_edges.to(device), train_neg_edges.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    embeddings = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Compute positive and negative scores\n",
    "    pos_scores = (embeddings[train_edges[0]] * embeddings[train_edges[1]]).sum(dim=1)\n",
    "    neg_scores = (embeddings[train_neg_edges[0]] * embeddings[train_neg_edges[1]]).sum(dim=1)\n",
    "    \n",
    "    # Compute loss  \n",
    "    loss = loss_fn(pos_scores, torch.ones_like(pos_scores)) + loss_fn(neg_scores, torch.zeros_like(neg_scores))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ GNN Link Prediction ROC-AUC Score: 0.7682\n",
      "üîπ GNN Link Prediction Accuracy: 0.9855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Ensure test edges are on the correct device\n",
    "test_edges, test_neg_edges = test_edges.to(device), test_neg_edges.to(device)\n",
    "\n",
    "# Ensure correct shape\n",
    "test_edges = test_edges.view(2, -1)\n",
    "test_neg_edges = test_neg_edges.view(2, -1)\n",
    "\n",
    "# Model evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "    # Compute positive and negative scores\n",
    "    pos_scores = (embeddings[test_edges[0]] * embeddings[test_edges[1]]).sum(dim=1)\n",
    "    neg_scores = (embeddings[test_neg_edges[0]] * embeddings[test_neg_edges[1]]).sum(dim=1)\n",
    "\n",
    "    # Ensure score dimensions are valid\n",
    "    assert pos_scores.dim() == 1, f\"Expected pos_scores to be 1D, but got {pos_scores.shape}\"\n",
    "    assert neg_scores.dim() == 1, f\"Expected neg_scores to be 1D, but got {neg_scores.shape}\"\n",
    "\n",
    "# Convert scores to probability predictions\n",
    "y_pred = torch.cat([pos_scores, neg_scores]).sigmoid().cpu().numpy()\n",
    "y_true = np.hstack([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Compute Accuracy Score\n",
    "accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "\n",
    "print(f\"üîπ GNN Link Prediction ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"üîπ GNN Link Prediction Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances\n",
    "- Clearly, our GNN far outshines our Node2Vec implementation in terms of accuracy (**98.55%** vs **82.91%**)\n",
    "- However, when it comes to ROC-AUC score, Node2Vec performed better, giving **0.8692** vs the GNN's **0.7682**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & Intuition Behind Feature Choices\n",
    "- I constructed two different types of features for the models:\n",
    "\n",
    "    - 1Ô∏è‚É£ Node2Vec Features\n",
    "        -  Encodes structural similarity by learning dense vector embeddings.\n",
    "        -  Captures graph topology‚Äîcountries frequently used in play will have closer embeddings.\n",
    "        -  Strength: Pretrained embeddings allow fast inference.\n",
    "        -  Limitation: Doesn't adapt dynamically to changes in the graph.\n",
    "\n",
    "    - 2Ô∏è‚É£ One-Hot Encoded Letter Features\n",
    "        -  Uses only the first and last letter of each country as features.\n",
    "        -  Simple but directly aligns with the Atlas game rule.\n",
    "        -  Strength: Rule-based and interpretable, less complex than deep embeddings.\n",
    "        -  Limitation: Ignores graph connectivity beyond first/last letter.\n",
    "\n",
    "    - 3Ô∏è‚É£ GNN Features\n",
    "        -  Learns latent structural relationships dynamically through message passing.\n",
    "        -  Uses both letter features & graph topology to predict links.\n",
    "        -  Strength: Adaptively learns new relationships.\n",
    "        -  Limitation: Computationally expensive, requires retraining when the graph updates.\n",
    "\n",
    "## Unsupervised Training Objective\n",
    "- Since our dataset lacks explicit labels, I trained both models in an unsupervised manner:\n",
    "\n",
    "    - Masked Edge Training\n",
    "\n",
    "        - We removed 20% of edges for validation.\n",
    "        - Model learned to reconstruct missing links.\n",
    "        - Tested whether the model correctly predicts masked edges.\n",
    "\n",
    "    - Negative Sampling (Node2Vec)\n",
    "\n",
    "        - Since not every pair of countries should be connected, we sampled negative edges.\n",
    "        - Trained classifier to distinguish real vs. non-existent edges.\n",
    "\n",
    "    - Contrastive Learning in GNNs\n",
    "\n",
    "        - Model learns to embed similar nodes closer together in representation space.\n",
    "        - Link prediction is performed by computing similarity between node embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
